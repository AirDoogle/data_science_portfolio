{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj6mOEoTsCJX"
   },
   "source": [
    "# CS6405 - Data Mining - First Assignment\n",
    "\n",
    "Lecturer: Dr Andrea Visentin\n",
    "\n",
    "\n",
    "## Submission\n",
    "\n",
    "This assignment is **due on 03/03/24 at 23:59**. You should submit a single .ipnyb file with your python code and analysis electronically via Canvas.\n",
    "Please note that this assignment will account for **25 Marks** of your module grade.\n",
    "\n",
    "## Declaration\n",
    "\n",
    "By submitting this assignment. I agree to the following:\n",
    "\n",
    "<font color=\"red\">“I have read and understand the UCC academic policy on plagiarism and I agree to the\n",
    "requirements set out thereby in relation to plagiarism and referencing. I confirm that I\n",
    "have referenced and acknowledged properly all sources used in preparation of this\n",
    "assignment.\n",
    "I declare that this assignment is entirely my own work based on my personal study. I\n",
    "further declare that I have not engaged the services of another to either assist me in, or\n",
    "complete this assignment”</font>\n",
    "\n",
    "## Specification\n",
    "\n",
    "The objective of this project is to build a k-Nearest Neighbour algorithm that takes as input training and test dataset and will predict the target binary variable with a reasonable degree of accuracy.\n",
    "\n",
    "You will find a template for your python code in Canvas along with the dataset. In this project, you must implement your own machine learning library and therefore **you are only allowed to use the following python packages and libraries: NumPy, Pandas.**\n",
    "\n",
    "This project focuses on the Titanic dataset that we used during the lectures. You can find the dataset at:\n",
    "\n",
    "https://github.com/andvise/DataAnalyticsDatasets/blob/main/titanic.csv\n",
    "\n",
    "\n",
    "# Tasks:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jkbPvQyvLHl"
   },
   "source": [
    "# Data Preparation (5 marks)\n",
    "\n",
    "\n",
    "\n",
    "1.  Load the dataset on Colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_0CEdmKwuYdw",
    "outputId": "e62ea5be-5fba-4204-b571-7de1e8ca6d6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  \\\n",
      "0                             Mr. Owen Harris Braund    male  22.0   \n",
      "1  Mrs. John Bradley (Florence Briggs Thayer) Cum...  female  38.0   \n",
      "2                              Miss. Laina Heikkinen  female  26.0   \n",
      "3        Mrs. Jacques Heath (Lily May Peel) Futrelle  female  35.0   \n",
      "4                            Mr. William Henry Allen    male  35.0   \n",
      "\n",
      "   Siblings/Spouses Aboard  Parents/Children Aboard     Fare  \n",
      "0                        1                      0.0   7.2500  \n",
      "1                        1                      0.0  71.2833  \n",
      "2                        0                      0.0   7.9250  \n",
      "3                        1                      NaN  53.1000  \n",
      "4                        0                      0.0   8.0500  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"https://github.com/andvise/DataAnalyticsDatasets/blob/dc74027bd063510ede437e4d612f41eb078e49b9/titanic.csv?raw=true\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4AfYHzAudy2"
   },
   "source": [
    "\n",
    "\n",
    "2. Display the attributes' name and their data type\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hfi7ERXDu6h2",
    "outputId": "839ff96a-fab3-4bf8-d26c-5b0665ee1f7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId                  int64\n",
       "Survived                     int64\n",
       "Pclass                       int64\n",
       "Name                        object\n",
       "Sex                         object\n",
       "Age                        float64\n",
       "Siblings/Spouses Aboard      int64\n",
       "Parents/Children Aboard    float64\n",
       "Fare                       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAX0YZxyutsm"
   },
   "source": [
    "3. Delete the columns that should not be included in a k-NN technique. Explain why you removed them in 1-2 sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_no2t3bEu7MY",
    "outputId": "a5869ce3-e915-4202-c9a8-9b07e4f4c527"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived                     int64\n",
       "Pclass                       int64\n",
       "Sex                         object\n",
       "Age                        float64\n",
       "Siblings/Spouses Aboard      int64\n",
       "Parents/Children Aboard    float64\n",
       "Fare                       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing columns that are not useful for k-NN\n",
    "df.drop(['PassengerId', 'Name'], axis=1, inplace=True)\n",
    "df.dtypes\n",
    "\n",
    "#PassengerId whas removed because its just an identifier for each row and doesnt hold meaningful information about a passengers survival\n",
    "#name was also removed , while it might not directly contribute to survival, it could be used to indicate titles or social status but for simiplicity and relevance for K-NN its removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAHTyJF8uys4"
   },
   "source": [
    "4. Replace all missing values with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FVo2V5jpu8td",
    "outputId": "e83b7728-634a-465d-cfb3-e3c376f0fa3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass     Sex   Age  Siblings/Spouses Aboard  \\\n",
      "0         0       3    male  22.0                        1   \n",
      "1         1       1  female  38.0                        1   \n",
      "2         1       3  female  26.0                        0   \n",
      "3         1       1  female  35.0                        1   \n",
      "4         0       3    male  35.0                        0   \n",
      "\n",
      "   Parents/Children Aboard     Fare  \n",
      "0                      0.0   7.2500  \n",
      "1                      0.0  71.2833  \n",
      "2                      0.0   7.9250  \n",
      "3                      0.0  53.1000  \n",
      "4                      0.0   8.0500  \n"
     ]
    }
   ],
   "source": [
    "df = df.fillna(0)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnCvSz_xIu4r"
   },
   "source": [
    "5. Transform the Sex column into a numerical one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kzkD32_8Iuih",
    "outputId": "b6dca711-d515-4673-819a-47ec3882fc90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass  Sex   Age  Siblings/Spouses Aboard  \\\n",
      "0         0       3    0  22.0                        1   \n",
      "1         1       1    1  38.0                        1   \n",
      "2         1       3    1  26.0                        0   \n",
      "3         1       1    1  35.0                        1   \n",
      "4         0       3    0  35.0                        0   \n",
      "\n",
      "   Parents/Children Aboard     Fare  \n",
      "0                      0.0   7.2500  \n",
      "1                      0.0  71.2833  \n",
      "2                      0.0   7.9250  \n",
      "3                      0.0  53.1000  \n",
      "4                      0.0   8.0500  \n"
     ]
    }
   ],
   "source": [
    "# Maps Sex column, male to 0 and female to 1, then convert to int\n",
    "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1}).astype(int)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vTfRa7_I0nr"
   },
   "source": [
    "6. Use **Survived** as the target label and the rest of the data frame as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "s1ru4us7I1Dg"
   },
   "outputs": [],
   "source": [
    "# Define the features and the target\n",
    "X = df.drop('Survived', axis=1)  # defines the features, all columns except 'Survived'\n",
    "y = df['Survived']  # defines the target label 'Survivied'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shSjCurZu0V6"
   },
   "source": [
    "5. Divide your dataset into 80% for training and 20% for test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lGRWLJkKu9fo"
   },
   "outputs": [],
   "source": [
    "# Generates random indices of length of the dataset\n",
    "import numpy as np\n",
    "indices = np.random.permutation(len(df))\n",
    "\n",
    "# 80/20 splits the random indices and assigns them to the train and test data\n",
    "train_size_80 = int(len(df) * 0.8)\n",
    "train_indices = indices[:train_size_80]\n",
    "test_indices = indices[train_size_80:]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train_set = X.iloc[train_indices]\n",
    "y_train_set = y.iloc[train_indices]\n",
    "X_test_set = X.iloc[test_indices]\n",
    "y_test_set = y.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMkhqPtzu1n3"
   },
   "source": [
    "6. Scale the columns using min-max scalers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0MzMQq9LTgrp"
   },
   "outputs": [],
   "source": [
    "# gets the max and min of the. training set\n",
    "X_min = X_train_set.min()\n",
    "X_max = X_train_set.max()\n",
    "\n",
    "#sclales the train and test set using the formula\n",
    "X_train_scaled_set = (X_train_set - X_min) / (X_max - X_min)\n",
    "X_test_scaled_set = (X_test_set - X_min) / (X_max - X_min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8Beg7r2u24u"
   },
   "source": [
    "7. Print the shape of the train and test set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GRmj29wdu_NR",
    "outputId": "75d946ce-b948-4424-b536-f4c787d18782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (709, 6)\n",
      "Test set shape: (178, 6)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the manually split train and test sets\n",
    "print(\"Training set shape:\", X_train_scaled_set.shape)\n",
    "print(\"Test set shape:\", X_test_scaled_set.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1f455v-vzXd"
   },
   "source": [
    "# NN implementation (10 Marks)\n",
    "\n",
    "1. Implement your NN method. To predict **each** point (query point) of the test set, you need to:\n",
    "\n",
    "\n",
    "> a. Find the training point with the smallest **Euclidian** distance  with the query point\n",
    "\n",
    "\n",
    "  \n",
    "> b. Use as a prediction the target value of the point with the smallest distance\n",
    "to the query point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWGWMI10wXJ5"
   },
   "source": [
    "2. Compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CXtkqLwQwZ_0"
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(row1, row2):\n",
    "    \"\"\"\n",
    "    Calculates the Euclidean distance between two points.\n",
    "\n",
    "    Parameters:\n",
    "    - row1: Numpy array, the first point.\n",
    "    - row2: Numpy array, the second point.\n",
    "\n",
    "    Returns:\n",
    "    - The Euclidean distance between the points.\n",
    "    \"\"\"\n",
    "    return np.sqrt(((row1 - row2) ** 2).sum(axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EEnCJgVIpU9Z"
   },
   "outputs": [],
   "source": [
    "def nearest_neighbor(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Calculates distances to all training points then produces a np.array of predictions using the smallest distance's(NN) target value\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: DataFrame, feature set of the scaled training data.\n",
    "    - y_train: Series, target values of the training data.\n",
    "    - X_test: DataFrame, feature set of the scaled test data.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: Predictions for the test set.\n",
    "    \"\"\"\n",
    "    # Stores the predictions for the test set in an empty list.\n",
    "    predictions = []\n",
    "\n",
    "    # Convert to Numpy for faster computation\n",
    "    X_train_np = X_train.to_numpy()\n",
    "    X_test_np = X_test.to_numpy()\n",
    "    y_train_np = y_train.to_numpy()\n",
    "\n",
    "    # Iterate over each test point\n",
    "    for test_point in X_test_np:\n",
    "        # Calculate Euclidean distances from this test point to all training points\n",
    "        distances = euclidean_distance(X_train_np, test_point)\n",
    "\n",
    "        # Find the index of the smallest distance (nearest neighbor)\n",
    "        nearest_index = distances.argmin()\n",
    "\n",
    "        # Use the target value of the nearest neighbor as the prediction\n",
    "        predictions.append(y_train_np[nearest_index])\n",
    "\n",
    "    return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lwrBvOkJrja6",
    "outputId": "89ee46bf-a1e5-4eab-b971-42b904918175"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Makes predictions for the test set\n",
    "y_pred = nearest_neighbor(X_train_scaled_set, y_train_set, X_test_scaled_set)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bKUMqwZyrlRy"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Accuracy function that compares predicted values to true values\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Series, actual target values.\n",
    "    - y_pred: np.array, predicted target values.\n",
    "\n",
    "    Returns:\n",
    "    - float: The accuracy of the predictions.\n",
    "    \"\"\"\n",
    "    #compares predictions with the true test labels\n",
    "    correct_predictions = (y_true == y_pred).sum()\n",
    "    accuracy = correct_predictions / len(y_true)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cw8raA-Q2NsX",
    "outputId": "bbd8e81d-b547-45aa-a343-df9ef6c06e07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7640449438202247\n"
     ]
    }
   ],
   "source": [
    "# Calculates the accuracy of the NN model\n",
    "accuracy = calculate_accuracy(y_test_set, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAejsmlnwbtq"
   },
   "source": [
    "# k-NN implementation (10 Marks)\n",
    "\n",
    "1. Extend the previous implementation by using the average of the k closest neighbours as a prediction\n",
    "\n",
    "2. Add the **Manhattan** and **Hamming** distance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pf5pF5V2xCAF"
   },
   "source": [
    "2. Compute the accuracy\n",
    "\n",
    "> a. Is it more accurate compared to the previous implementation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "B9jUygeExFdv"
   },
   "outputs": [],
   "source": [
    "def manhattan_distance(row1, row2):\n",
    "    \"\"\"\n",
    "    Calculate the Manhattan distance between two points.\n",
    "\n",
    "    Parameters:\n",
    "    - row1: Numpy array, the first point.\n",
    "    - row2: Numpy array, the second point.\n",
    "\n",
    "    Returns:\n",
    "    - The Manhattan distance between the points.\n",
    "    \"\"\"\n",
    "    return np.abs(row1 - row2).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "x1_AMHb-exQz"
   },
   "outputs": [],
   "source": [
    "def hamming_distance(row1, row2):\n",
    "    \"\"\"\n",
    "    Calculate the Hamming distance between two points.\n",
    "\n",
    "    Parameters:\n",
    "    - row1: Numpy array, the first point.\n",
    "    - row2: Numpy array, the second point.\n",
    "\n",
    "    Returns:\n",
    "    - The Hamming distance between the points.\n",
    "    \"\"\"\n",
    "    return (row1 != row2).astype(int).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BRyjDwTLRRWh"
   },
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(X_train, y_train, X_test, k, distance_fn):\n",
    "    \"\"\"\n",
    "    Predict the target value for each test point based on the k nearest neighbors in the training set.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: DataFrame, feature set of the training data.\n",
    "    - y_train: Series, target values of the training data.\n",
    "    - X_test: DataFrame, feature set of the test data.\n",
    "    - k: int, number of nearest neighbors to consider.\n",
    "    - distance_fn: function, the distance function to use.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: Predictions for the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    # Stores the predictions for the test set in an empty list.\n",
    "    predictions = []\n",
    "\n",
    "    #Convert to Numpy for faster computation\n",
    "    X_train_np = X_train.to_numpy()\n",
    "    X_test_np = X_test.to_numpy()\n",
    "    y_train_np = y_train.to_numpy()\n",
    "\n",
    "    # Iterate over each test point\n",
    "    for test_point in X_test_np:\n",
    "        #calculate the specified distance measure from this test point to all training points\n",
    "        distances = distance_fn(X_train_np, test_point)\n",
    "\n",
    "        # Gets the indices of the k nearest neighbours\n",
    "        nearest_indices = distances.argsort()[:k]\n",
    "\n",
    "        # Gets the corresponding target values for these indices\n",
    "        nearest_labels = y_train_np[nearest_indices]\n",
    "\n",
    "        # Determines the most common label (class) among the nearest neighbours\n",
    "        prediction = np.bincount(nearest_labels).argmax()\n",
    "\n",
    "        # Appends the prediction to the predictions list\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZYfM45hKTj1"
   },
   "source": [
    "3. Test your approach for k  values = [1, 3, 5, 7] and the three distance measures\n",
    "\n",
    "> a. Which is the best combination?\n",
    "\n",
    "> b. How can you improve the results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHjue8FyKS9J",
    "outputId": "5bf39d86-606f-4f00-c230-eda5e97c19dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7752808988764045, when k value = 1 using euclidean_distance measure\n",
      "Accuracy = 0.8314606741573034, when k value = 3 using euclidean_distance measure\n",
      "Accuracy = 0.8370786516853933, when k value = 5 using euclidean_distance measure\n",
      "Accuracy = 0.8539325842696629, when k value = 7 using euclidean_distance measure\n",
      "Accuracy = 0.7752808988764045, when k value = 1 using manhattan_distance measure\n",
      "Accuracy = 0.848314606741573, when k value = 3 using manhattan_distance measure\n",
      "Accuracy = 0.848314606741573, when k value = 5 using manhattan_distance measure\n",
      "Accuracy = 0.8651685393258427, when k value = 7 using manhattan_distance measure\n",
      "Accuracy = 0.8146067415730337, when k value = 1 using hamming_distance measure\n",
      "Accuracy = 0.8202247191011236, when k value = 3 using hamming_distance measure\n",
      "Accuracy = 0.8146067415730337, when k value = 5 using hamming_distance measure\n",
      "Accuracy = 0.8146067415730337, when k value = 7 using hamming_distance measure\n"
     ]
    }
   ],
   "source": [
    "# Define the k values and distance measures\n",
    "k_values = [1, 3, 5, 7]\n",
    "distance_measures = [euclidean_distance, manhattan_distance, hamming_distance]\n",
    "\n",
    "# Iterate over each distance measure\n",
    "for distance in distance_measures:\n",
    "    distance_name = distance.__name__  # Gets the name of the function for printing clarity\n",
    "    # Iterate over each k value\n",
    "    for k in k_values:\n",
    "        # Compute predictions using k-NN\n",
    "        y_pred_knn = k_nearest_neighbors(X_train_scaled_set, y_train_set, X_test_scaled_set, k, distance)\n",
    "\n",
    "        # Compute accuracy\n",
    "        accuracy_knn = calculate_accuracy(y_test_set, y_pred_knn)\n",
    "\n",
    "        # Print accuracies for comparison\n",
    "        print(f\"Accuracy = {accuracy_knn}, when k value = {k} using {distance_name} measure\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_RynVceQJ1e"
   },
   "source": [
    "## Question 2a, Is it more accurate compared to the previous implementation?\n",
    "Yes the KNN model is more accurate than the previous NN model across all combinations, the previous implementation of the NN model returned an Accuracy: 0.7640449438202247, however every iteration of the KNN model output a higher accuracy with the lowest Accuracy = 0.7752808988764045, when k value = 1 using euclidean_distance measure.\n",
    "\n",
    "## Question 3a, Which is the best combination?\n",
    "When k value = 7 using manhattan_distance measure, this gave the highest Accuracy = 0.8651685393258427\n",
    "\n",
    " however other combinations with varying K using Manattan and Euclidiean also showed improved accuracy in different instances, this could be due to the random splitting of the data sets being favoured by other combinations.\n",
    "\n",
    "## Question 3b, How can you improve the results?\n",
    "To improve results the following approaches could be taken:\n",
    "1. Feature engineering: transforming features could give more meaningful inputs into the model, one feature that could be transformed are the names, instead of excluding this data from the model, class and titles could be be extracted and new features based on combined attributes could provide new insights and improve results.\n",
    "\n",
    "2. Data preprocessing: Further analysing the preprocessing steps could provide more meaningful data to improve the models results, this could involve using different methods to handle missing data values, scaling, normalisation standardisation techniques as these have a significant impact on KNN results.\n",
    "Properly encoding categorical variables.\n",
    "\n",
    "3. Parameter tuning: Using different values of k differing to the tested ones, this could yield improved results, a weighted KNN could also improve results with closer points having more influence on the predictions\n",
    "\n",
    "4. Distance Measure: other measures could improve results, such as Minkowski or Cosine [1].\n",
    "\n",
    "5. Cross Validation: Using cross validation could improve results by evaluating the models performance better and ensures the model generalises well across difference subsets of the data [1].\n",
    "\n",
    "6. Address imbalance data: if data is unbalanced,oversampling the minority class and undersampling the majority class can be used, however the target variable is relatively equally distributed with  a 61:39 split which means its reasonably balanced  Survived: 0    0.609309,\n",
    "1    0.390691 [1].\n",
    "\n",
    "7. Ensemble techniques: combining multiple models can yield better results than from a single KNN model [1].\n",
    "\n",
    "8. Dimensionality Reduction: using techniques such as PCA principical component analysis where dimensionality is reduced without losing a lot of information which can prevent the curse of Dimensionality and prevent overfitting [1].\n",
    "\n",
    "Reference: [1] btd. (2023). \"Strategies for k-NN Classifier Fine-Tuning\". [*Medium.com*]( https://baotramduong.medium.com/strategies-for-knn-classifier-fine-tuning-f225b873da4a )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFj4rxoQxJE8"
   },
   "source": [
    "**Reminder: You can not use any library, with the exception of pandas and NumPy. So scikit-learn is forbidden!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ux-5q5AOyGfo"
   },
   "source": [
    "## Marking Scheme\n",
    "**Program Correctness:** Your program should work correctly on all inputs (including new datasets). Also, if there are any specifications about how the program should be written, or how the output should appear, those specifications should be followed.\n",
    "\n",
    "**Readability:** Variables functions should have meaningful names. The code should be organised into functions/methods where appropriate. There should be an appropriate amount of white space so that the code is readable, and the indentation should be consistent.\n",
    "\n",
    "**Documentation:** your code and functions/methods should be appropriately commented. However, not every line should be commented because that makes your code overly busy. Think carefully about where comments are added.\n",
    "\n",
    "\n",
    "**Code Elegance:** There are many ways to write the same functionality into your code, and some of them are needlessly slow or complicated. For example, if you are repeating the same code, it should be inside creating a new method/function or for loop\n",
    "\n",
    "**Code efficiency:** The implementation is logically well-designed without inappropriate\n",
    "design choices (e.g., unnecessary loops)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
